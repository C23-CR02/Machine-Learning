# -*- coding: utf-8 -*-
"""Anomaly Pipeline CloudRaya Machine Learning.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/13msHSNKbOj0pZin3F_gEg-4y09B1lPgZ
"""

# from google.colab import drive
# drive.mount('/content/drive')

# Commented out IPython magic to ensure Python compatibility.
import pandas as pd
import numpy as np
import sklearn
import matplotlib.pyplot as plt
import ast
import warnings
from statsmodels.tsa.arima.model import ARIMA
from sklearn.metrics import mean_squared_error
from sklearn.metrics import mean_absolute_error
from itertools import groupby
from datetime import datetime, timedelta
from sklearn import metrics
from sklearn.model_selection import train_test_split

warnings.filterwarnings("ignore")
pd.options.mode.chained_assignment = None  # default='warn'
# %matplotlib inline

"""## Converting JSON Data into CSV"""

full_data_json = pd.read_json('response1.json')  # Data output dari Get API

res_list = []

for i in range(len(full_data_json)):
  res = full_data_json["data"][i]
  res_list.append(res)

json_full = pd.DataFrame.from_dict(res_list)[['date', 'hour', 'cpu_used']]
json_full['created_at'] = pd.to_datetime(json_full['date'])
json_full['created_at'] = json_full['created_at'] + pd.to_timedelta(json_full['hour'], unit='h')
json_full['cpu_used'] = json_full['cpu_used'] / 100
full_data = json_full[['created_at', 'cpu_used']]

"""### Train-Test Split"""

# Splitting the dataset
x_train, x_test, y_train, y_test = train_test_split(full_data.iloc[:, :-1], full_data.iloc[:, -1], 
                                                    test_size=0.2, shuffle=False)

"""## Modeling: ARIMA Univariate Time Series Forecasting

### ARIMA Model Hyperparameter Tuning
"""

# Evaluate an ARIMA model for a given order (p,d,q)
def evaluate_arima_model(df_train_y, df_test_y, arima_order):
    # Prepare training dataset
    train_size = int(len(df_train_y))
    test_size = int(len(df_test_y))
    train, test = df_train_y, df_test_y
    # Make predictions
    model = ARIMA(df_train_y, order=arima_order)
    model_fit = model.fit()
    predictions = model_fit.forecast(test_size)
    # Calculate out of sample error
    rmse = (mean_squared_error(test, predictions))**0.5
    mae = mean_absolute_error(test, predictions)
    return rmse, mae
 
# Evaluate combinations of p, d and q values for an ARIMA model
def evaluate_models(df_train_y, df_test_y, p_values, d_values, q_values):
    best_score, best_cfg, best_mae = float("inf"), None, float("inf")
    for p in p_values:
        for d in d_values:
            for q in q_values:
                order = (p,d,q)
                try:
                    rmse, mae = evaluate_arima_model(df_train_y, df_test_y, order)
                    if rmse < best_score:
                        best_score, best_cfg, best_mae = rmse, order, mae
                    print('ARIMA%s RMSE=%.7f MAE=%.7f' % (order,rmse,mae))
                except:
                    continue
    return best_cfg

# HATI-HATI LAMA
# Evaluate parameters

p_values = [1, 2, 3, 4]
d_values = [0, 1]
q_values = [1, 2, 3, 4]

p, d, q = evaluate_models(y_train, y_test, p_values, d_values, q_values)

"""#### Forecasting Future Values of CPU Usage"""

model = ARIMA(full_data['cpu_used'], order=(p,d,q))
model_fit = model.fit()

# Set the number of days to be forecasted
forecasted_days = 1

# Specify the starting timestamp
latest_timestamp = full_data.iloc[-1, 0]
interval = timedelta(minutes=60)

# Specify the number of times to increment the timestamp
num_times = int(forecasted_days * 24 * (60/60))

# Create an empty list to store the timestamps
timestamps = []

# Generate the timestamps
for i in range(1, num_times+1):
    timestamps.append(latest_timestamp + i * interval)

# Self-predict the exiting data with the trained-model
fitting = model_fit.predict(start=0, end=len(full_data)-1)


# Forecasting new data
forecasts = model_fit.forecast(int(forecasted_days * 24 * (60/60))).T

# Outputting the forecasted data (NECESSARY DATA [PROBABLY])
forecasts = pd.DataFrame({'Timestamp': timestamps, 'Forecasts': forecasts})

forecasts_json=forecasts.to_json(orient='records')

"""## Anomaly Detection"""

df_dummy = full_data.copy()
df_dummy["fittings"] = fitting
df_dummy['Error'] = df_dummy['cpu_used'] - df_dummy['fittings']

"""### Dynamic Thresholding + Consecutive Occurences"""

data_copy = df_dummy.copy()
std_coef = 1.5  # Besar standar deviasi penyimpangan dari mean window
window = int(6)  # Windowing
consecutive = 3  # Frekuensi minimum kemunculan outliers berturut-turut

data_copy['mean'] = pd.Series(data_copy['Error'].rolling(window=window).mean())
data_copy['std'] = pd.Series(data_copy['Error'].rolling(window=window).std())
data_copy['up_thres'] = pd.Series(data_copy['Error'].rolling(window=window).mean()) \
                    + (std_coef * pd.Series(data_copy['Error'].rolling(window=window).std()))
data_copy['down_thres'] = pd.Series(data_copy['Error'].rolling(window=window).mean()) \
                    - (std_coef * pd.Series(data_copy['Error'].rolling(window=window).std()))

out_index = data_copy.index[(data_copy['Error'] > data_copy['up_thres']) | (data_copy['Error'] < data_copy['down_thres'])]

data_copy['outliers_bool'] = [False for _ in range(len(data_copy))]
data_copy['outliers_bool'][out_index] = True

greater_th = [list(g) for k, g in groupby(data_copy['outliers_bool']==True)]

for i in range(len(greater_th)):
  if greater_th[i].count(True) < consecutive:
    greater_th[i] = [False for _ in greater_th[i]]

greater_th = pd.DataFrame({"Outliers": [element for sublist in greater_th for element in sublist]})
updated_out_index = greater_th[greater_th["Outliers"]==True].index

data_anomaly_labeled = data_copy[['created_at', 'cpu_used']]
data_anomaly_labeled['is_anomaly'] = [False for _ in range(len(data_anomaly_labeled))]
data_anomaly_labeled['is_anomaly'].iloc[updated_out_index] = True
data_anomaly_labeled['cpu_used'] = (data_anomaly_labeled['cpu_used']*100).round(4).astype(str) + "%"

# Saving Anomaly Labeled Data
data_anomaly_labeled.to_csv('anomaly_detection.csv', index=False)

data_anomaly_labeled_json = data_anomaly_labeled.to_json(orient="records")

last_data = data_anomaly_labeled.tail(1)
is_anomaly_value = last_data['is_anomaly'].values[0]
is_anomaly_value